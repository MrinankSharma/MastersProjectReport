% ******************************* Introduction ********************************
\chapter{Introduction}
%
%\ifpdf
%\graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
%\else
%\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
%\fi
\graphicspath{{Introduction/Figs}}

Machine learning methods are trained on large quantities of data, leveraging information within the dataset in order to make predictions about previously unseen data and make decisions. Recently, such methods have found use in scenarios where the data used in personal and sensitive, one example being the use of human genomic data to predict drug sensitivity \cite{drugSensitivity}. Typically, data relating to individuals in training datasets are \emph{anonymised}, for example, by removing all identifiable information (such as names, addresses, etc) and replacing this information with an anonymous identifier. However, Narayanan and Shmatikov show that anonymisation is insufficient, partially due to the availability of \emph{auxiliary information} i.e. additional, publicly available information. When Netflix released a dataset in 2006 containing movie ratings for approximately $500000$ subscribers with names replaced with identifiers, the data could be combined with public ratings on Internet Movie Database to identify movie ratings of two users. \cite{netflix} Intuitively, data-points about individuals are highly dimensional meaning that anonymisation is insufficient, a further example being that even when sharing DNA sequence data without identifiers, it is possible to recover particular surnames using additional metadata. \cite{gymrek2013identifying}\mynote{Look into these papers more} Whilst a particular user's film ratings are not particularly sensitive, a lack of privacy in the areas of healthcare and public policy are critical.  

Fredrikson et. al have shown that \emph{model inversion attacks} are possible, where an adversary seeks to learn information about training data given model predictions. In particular, a neural network for facial recognition which returned confidence values was exploited in order to recover the image of a training set participant. Whilst more sophisticated attacks will be required for algorithms which do not provide confidence information, it is therefore possible for an adversary to recover anonymised training data-points and then de-anonymise this information using auxiliary information. \cite{modelInversion} 

The increasing availability and affordability of mobile smart-phones means that the \emph{federated learning} context, where data across a number of clients is used to train a global model without transferring local data to a central server, is particularly interesting. Additionally, federated learning schemes reduce power consumption and intuitively give stronger privacy by removing the requirement of transferring entire local datasets. \cite{google_ai_blog_2017} Additionally, in order to make optimal decisions, it is important to model the uncertainty in what is known. This corresponds to performing Bayesian inference and producing a \emph{posterior distribution} over unknown variables.

\begin{figure}
	\includestandalone[width=0.6\textwidth]{Introduction/Figs/TikZ/venn}
	\label{fig:intro:venn}
	\centering
	\caption{Project aims, including other work within this area. }
\end{figure} 
\mynote{Update Venn Diagram - also add these papers in the references of this report}

This project aims to develop a generalised method to enable Bayesian inference to be performed in contexts where data is distributed over a number of clients whilst also providing privacy guarantees for each client. 
