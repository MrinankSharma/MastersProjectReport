% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
Machine learning models have recently found use in contexts in which large quantities of private, sensitive data are used to make decisions and predictions, namely healthcare and public policy. It is essential to ensure that an adversary is unable to infer private sensitive information from model predictions. Differential Privacy, originally developed in cryptography literature, has become a mathematical standard for quantifying the level of privacy offered by parties processing data by introducing carefully calibrated noise when data queries are made. A particular area of interest is the federated learning context in which a number of clients, each of which holds a local dataset, interact with a central parameter server in order to train a shared model. The aim of this project is to develop techniques to allow for differentially private federated Bayesian learning.

Partitioned Variational Inference (PVI) is a recently developed framework which enables variational inference to be applied in the federated learning context. In this report, we present two adaptations to the PVI algorithm for parametric families which incorporate differential privacy techniques in order to protect the privacy of individuals. The first adaptation, data-point level DP-PVI, limits and obscures the contribution of each individual data-point held by every client and is the natural technique to apply when the data from each client could relate to a number of individuals and does not require encryption and authentication schemes. The second adaptation, dataset level DP-PVI, limits and obscures the contribution that a client's entire dataset has on the global model parameters and is natural when the dataset from each client relates to one individual only but requires message encryption. Both of these techniques rely on clipping, which limits how much individual data-points (or data-sets) are able to influence outcomes. The algorithms developed are fully general and could be applied to a large range of probabilistic models.

The case study of Bayesian linear regression, using a Gaussian prior and likelihood, is developed to investigate the performance and understand the properties of these algorithms. We find that across both techniques, extreme care must be taken in choosing algorithm hyper-parameters, specifically the clipping bound which can introduce significant bias if chosen inappropriately. We find the dataset level DP-PVI algorithm can give very close to non-private performance and is the more promising of the approaches applied. Furthermore, we note that the choice of parametrisation is very important and has significant implications for the outcomes of this algorithm. 
\end{abstract}
