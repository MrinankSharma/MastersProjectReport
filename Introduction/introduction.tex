% ******************************* Introduction ********************************
\chapter{Introduction}
%
%\ifpdf
%\graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
%\else
%\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
%\fi
\graphicspath{{Introduction/Figs}}

Machine learning methods are trained on datasets, leveraging information within the dataset in order to make predictions about previously unseen data and make decisions. Recently, such methods have found use in scenarios where the data used is personal and sensitive, one example being the use of human genomic data to predict drug sensitivity \citep{drugSensitivity}. Typically, data relating to individuals in training datasets are \emph{anonymised}, for example, by removing all identifiable information (such as names, addresses, etc) and replacing this information with an anonymous identifier. However, \cite{netflix} show that anonymisation is insufficient, partially due to the availability of \emph{auxiliary information} i.e. additional, publicly available information. When Netflix released a dataset in 2006 containing movie ratings for approximately $500000$ subscribers with names replaced with identifiers, the data could be combined with public ratings on Internet Movie Database (IMDb) to identify movie ratings of two users. Intuitively, data-points about individuals are highly dimensional meaning that anonymisation is insufficient, a further example being that even when sharing DNA sequence data without identifiers, it is possible to recover particular surnames using additional meta-data \citep{gymrek2013identifying}. Machine learning approaches have also been used in public policy, for instance geographically placing refugees to optimise their overall employment rate \citep{bansak2018improving}. 

\citet{modelInversion} have shown that \emph{model inversion attacks} are possible, where an adversary seeks to learn information about training data given model predictions. In particular, a neural network for facial recognition which returned confidence values was exploited in order to recover the image of a training set participant. Whilst more sophisticated attacks will be required for algorithms which do not provide confidence information, it is therefore possible for an adversary to recover anonymised training data-points and then de-anonymise this information using auxiliary information. 

\emph{Federated learning}, which performs global model training using a large quantity of de-centralised data, is a particularly interesting context, especially with the increasing availability and affordability of mobile smart-phones. These approaches are also applicable for Internet of Things (IoT) devices. Additionally, federated learning schemes reduce power consumption and intuitively give stronger privacy by removing the requirement of transferring entire local datasets \citep{google_ai_blog_2017}. 

It is often desirable to performance \emph{Bayesian Inference} when possible. Bayesian approaches incorporate prior information about unknown parameters in order to mathematically quantify the uncertainty in parameter estimates and predictions. This allows the application of decision theory, thus providing a framework to make optimal decisions under uncertainty \citep{Bishop:2006}.

\begin{figure}
	\includestandalone[width=0.6\textwidth]{Introduction/Figs/TikZ/venn}
	\label{fig:intro:venn}
	\centering
	\caption{Project aims, including other work within this area. }
\end{figure} 

This project lies in the intersection of privacy, Bayesian inference and federated learning. There has been limited previous work in the intersection of these yields, and current techniques are only applicable for exponential conjugate models \citep{heikkila2017differentially}. The aim of this project is to develop a generalised method to enable Bayesian inference to be performed using complex models in contexts where data is distributed over a number of clients whilst also providing privacy guarantees for each client. 
