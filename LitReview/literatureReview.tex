% ******************************* Literature Review ********************************

\chapter{Literature Review}
%
%\ifpdf
%\graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
%\else
%\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
%\fi
\graphicspath{{LitReview/Figs/}}

\section{Differential Privacy}
\subsection{Preliminaries}
Differential privacy is one mathematical technique, widely adopted by the community, which formalises privacy and is able to numerically quantify the level of privacy that some method provides. This is particularly useful not only from the point of view of a designer who is able to quantify the trade-off between privacy and utility but also from the point of view of a client whose data we seek to protect; a client would be able to compare the protection offered by rival companies, or parameter settings. 

\begin{definition}[$\epsilon$-Differential Privacy]
	A randomised algorithm, $\altmathcal{A}$, is said to be $\epsilon$-differentially private if for any possible subset of outputs, $S$, and for all pairs of datasets, $(\altmathcal{D, D'})$, which differ in one entry only, the following inequality holds:
	\begin{equation}
	\text{Pr}(\altmathcal{A(D)} \in S) \leq e^\epsilon \text{Pr}(\altmathcal{A(D')} \in S) 
	\end{equation}
\end{definition}

Noting that since this definition is symmetric across datasets, this requirement effectively enforces that:
\begin{align}
e^{-\epsilon} \leq \frac{\text{Pr}(\altmathcal{A(D)} \in S)}{\text{Pr}(\altmathcal{A(D')} \in S)} \leq e^{\epsilon}
\end{align}
Thus, it can be seen that this enforces privacy in the sense that the output probability densities ought to be similar (with the ratio of density values close to one) for datasets which are also similar (i.e. differ in only entry only) \citep{foundations}. $\epsilon$, a positive quantity, quantifies the level of privacy provided, with smaller values of epsilon corresponding to stronger privacy guarantees.

It is difficult to choose and interpret the value of $\epsilon$. \cite{hsu2014differential} provide a principled technique to choose $\epsilon$ for surveys based on economic analysis based on estimating the monetary cost of leaking private data. This is less appropriate for machine learning methods, but we note that common values of $\epsilon$ found in the literature (as found in \citet{hsu2014differential}) do not exceed $\epsilon = 10$.

In practice, differential privacy is achieved by bounding the contribution that any single data-point may have on the outcome and then adding noise which scales with the maximum magnitude of this contribution. The adversary would then be unable to determine whether a particular output was simply due to noise or due to the contribution of a specific data-point. This is related to the concept of plausible denialability; a client could simply claim that a certain outcome only occurred due to the noise applied rather than their specific contribution.

Differential privacy is suitable to quantify privacy as it is immune to post-processing; an adversary is only able to make the output of a differentially private algorithm less private (i.e. increase the value of $\epsilon$) with additional knowledge of the dataset \citep{foundations}. Additional knowledge of the dataset could refer to either specific information about data-points in the dataset or aggregate information about a collection of data-points in the dataset. 

Often, the above definition of differential privacy is slackened by introducing an extra privacy variable. 

\begin{definition}[$(\epsilon, \delta)$-Differential Privacy]
	\label{def:epdp}
	 A randomised algorithm, $\altmathcal{A}$, is said to be $(\epsilon, \delta)$ differentially private if for any possible subset of outputs, $S$, and for all datasets, $(\altmathcal{D, D'})$, which differ in one entry only, the following inequality holds:
	\begin{equation}
	\text{Pr}(\altmathcal{A(D)} \in S) \leq e^\epsilon \text{Pr}(\altmathcal{A(D')} \in S) + \delta
	\end{equation}
\end{definition}
Similar to $\epsilon$, larger values of $\delta$ correspond to weaker privacy guarantees and $\delta=0$ corresponds to a pure $\epsilon$-differentially private algorithm. Note that it can be shown that $(\epsilon, \delta)$-DP is equivalent to a probabilistic pure $\epsilon$-DP guarantee with probability $1-\delta$  \citep{foundations}.

A useful quantity regarding functions which operate on data is the $\ell_2$ sensitivity. 
\begin{definition}[$\ell_2$ Sensitivity]
	The $\ell_2$ sensitivity of function $f: \altmathcal{D} \rightarrow \mathbb{R}^n$, is denoted as $\Delta_2(f)$ and is defined as:
	\begin{align}
		\Delta_2(f) = \max_{\altmathcal{D, D'}} || f(\altmathcal{D}) - f(\altmathcal{D}')||_2
	\end{align}
	where $\altmathcal{D}$ and $\altmathcal{D'}$ differ in one entry only. 
\end{definition}
For a function with a given $\ell_2$ sensitivity, the Gaussian mechanism can be used to provide an $(\epsilon, \delta)$-differential privacy guarantee \citep{foundations}. 
\begin{theorem}[Gaussian Mechanism]
	Let $f: \altmathcal{D} \rightarrow \mathbb{R}^n$ be a function with $\ell_2$ sensitivity $\Delta_2(f)$. Releasing $f(\altmathcal{D}) + \eta$ is $(\epsilon, \delta)$-differentially private where $\eta \sim \altmathcal{N}(0, \sigma^2)$ when:
	\begin{equation}
	\sigma^2 > 2 \ln (1.25/ \delta) \Delta_2^2(f)/\epsilon^2
	\label{eq:litreview-GaussianMech}
	\end{equation}
	with $\epsilon \in (0,1)$.
\end{theorem}

Typically, a particular process may be repeatedly applied to a dataset; a simple example being that in many machine learning problems, a loss function can be composed into a sum over data-points. Gradient descent techniques repeatedly compute the gradient of the loss over subsets of data-points and use this estimate to adjust model parameters to reduce the loss \citep{ruder2016overview}. A key reason that differential privacy is an appropriate way of measuring privacy is that it can be \emph{composed}; when a randomised algorithm with some privacy guarantee is repeatedly applied to a dataset, it is possible to convert the individual privacy guarantees into an overall guarantee (note that the privacy guarantee refers to a $(\epsilon, \delta)$ value). There exist many schemes which bound the values of $\epsilon$ and $\delta$ in these circumstances. The \emph{Moments Accountant} is an advanced technique which is able to `account' for and track the total privacy expenditure of a technique by tracking the moments of the privacy loss and provides tight upper bounds on the values of $\epsilon$ and $\delta$. 

\subsection{The Moments Accountant}
The Moments Accountant was proposed in \citep{moments_account}. Here, we provide a summary of its computation following the original paper. 

\begin{definition}[Privacy Loss] For neighbouring datasets, $\altmathcal{D}$ and $\altmathcal{D'}$, a stochastic algorithm, $\altmathcal{A}$ and some outcome, $S$, define the privacy loss at $S$ as:
	\begin{equation}
	c(S; \altmathcal{A}, \altmathcal{D, D'}) \triangleq \log \frac{\Pr[ \altmathcal{A}( \altmathcal{D})=S]}{\Pr[ \altmathcal{A}(\altmathcal{D'}) = S]}
	\end{equation}
\end{definition}
If the probability of an observed algorithm outcome is very different across neighbouring datasets, observing this outcome reveals information about the dataset. This intuition matches the above definition, as this situation corresponds to a large (absolute) value of $c$. Note that since the algorithms we use are stochastic, the privacy loss is itself a random variable. It is clear that an $(\epsilon, 0)$ privacy guarantee directly corresponds to claiming that the privacy loss random variable will never exceed $\epsilon$ in absolute value.

\begin{definition}[$\lambda$th Moment of $\altmathcal{A}$]
	For algorithm $\altmathcal{A}$, a quantity of interest is the maximum value of the log of the moment generating function of the privacy loss:
	\begin{align}
	\alpha_\altmathcal{A}(\lambda) \triangleq \max_{\altmathcal{D, D'}} \ln \mathbb{E}_{S \sim \altmathcal{A(D)}} \Big \lbrace \exp \lambda c(S; \altmathcal{A}, \altmathcal{D, D'}) \Big \rbrace
	\end{align}
\end{definition}

\begin{theorem}[Properties of $\alpha_\altmathcal{A}(\lambda)$]
	$\alpha_\altmathcal{A}(\lambda)$ exhibits two important properties: \begin{enumerate}
		\item \textbf{Composability:} If the algorithm $\altmathcal{A}$ consists of a sequence of adaptive steps, $\altmathcal{M}_1, \ldots, \altmathcal{M}_k$, for any $\lambda$:
		\begin{equation}
		\alpha_\altmathcal{A}(\lambda) \leq \sum_{i=1}^k \alpha_{\altmathcal{M}_i}(\lambda)
		\end{equation}
		
		\item \textbf{Tail Bound:} For any $\epsilon > 0$ and $\lambda$, the stochastic algorithm, $\altmathcal{A}$, is $(\epsilon, \delta)$-differentially private for
		\begin{equation}
		\delta \leq \exp(\alpha_{\altmathcal{A}}(\lambda) - \lambda \epsilon)
		\end{equation}
	\end{enumerate}
\end{theorem}
The Moments Accountant scheme computes $\alpha_{\altmathcal{M}_i}(\lambda)$ for each step for several values of $\lambda$ and uses the composability property to compute the aggregate loss. In typical applications, either a target value for $\epsilon$ or $\delta$ is fixed at some value, and the tail bound property is applied to compute the best possible value of the other privacy variable, using one of the following equations:
\begin{align}
\delta &= \min_\lambda \exp(\alpha_{\altmathcal{A}}(\lambda) - \lambda \epsilon) \\
\epsilon &= \min_\lambda \frac{1}{\lambda} (\alpha_{\altmathcal{A}}(\lambda) - \ln \delta)
\end{align}
The minimum is taken as the tail bound property gives upper bounds on the privacy variables, but smaller values of $\epsilon$ and $\delta$ correspond to tighter bounds on the privacy loss and thus stronger privacy guarantees.

Let $\altmathcal{D} = \lbrace \bm{x}_i \rbrace$ and $\altmathcal{D}' = \altmathcal{D} \cup \bm{x}'$ where $\bm{x} \in \altmathcal{X}$. Let $f: \altmathcal{X} \rightarrow \mathbb{R}^n$ with $||f(\cdot)||_2 \leq \Delta$. Consider the following mechanism:
\begin{align}
\altmathcal{M}(\altmathcal{D}) &= \sum_{i \in J} f(\bm{x}_i) + \sigma \Delta \bm{\eta} \\ 
\eta_i &\sim \altmathcal{N}(0, 1) \nonumber
\end{align}
where $J$ is a subset of indices where each index is chosen independently with probability $q$. Without loss of generality, let $f(\bm{x}_i) = \bm{0}$ and $f(\bm{x}') = \Delta\cdot\bm{e}_1$ where $\bm{e}_1$ is a unit vector. Then $\altmathcal{M(D)}$ and  $\altmathcal{M(D')}$ are distributed identically other than the coordinate corresponding to $\bm{e}_1$. Considering only this direction, the output densities for the mechanism are:
\begin{align}
p(\altmathcal{M(D)}_1 = z )\sim \mu_0(z) &\triangleq \altmathcal{N}(z| 0, \Delta^2 \sigma^2) \\ p(\altmathcal{M(D)'}_1 = z) \sim \mu_1(z) &\triangleq q\cdot\altmathcal{N}(z| \Delta, \Delta^2 \sigma^2) + (1-q)\cdot\altmathcal{N}(z| 0, \Delta^2 \sigma^2) 
\end{align}
since the probability of $\bm{x}'$ being selected is $q$. Then, by definition, $\alpha_\altmathcal{M}(\lambda)$ can be computed as:
\begin{align}
\alpha_\altmathcal{M}(\lambda) &= \ln \max (E_1, E_2) \\
E_1 &= \mathbb{E}_{z \sim \mu_0} \Bigg[ \Big(\frac{\mu_0(z)}{\mu_1(z)}\Big)^\lambda \Bigg] \\
E_2 &= \mathbb{E}_{z \sim \mu_1} \Bigg[ \Big(\frac{\mu_1(z)}{\mu_0(z)}\Big)^\lambda \Bigg]
\end{align}
Each integral can be evaluated using numerical integration. Both integrals must be considered since the maximum of the log moments of the privacy loss must be found, and both $\altmathcal{D}$ and $\altmathcal{D}'$ could be considered as the `original' dataset.

\textbf{Note: }the computed values of $\epsilon$ and $\delta$ are independent of the data used for the mechanism and can be pre-computed given fixed values of $\Delta$, $q$ and either $\delta$ or $\epsilon$. 

\subsection{Differentially Private Stochastic Gradient Descent}
\cite{moments_account} propose differentially private stochastic gradient descent by adapting stochastic gradient descent to use the Gaussian mechanism with additional sub-sampling to compute the gradient of some loss function. 

Algorithm \ref{alg:DP-SGD} outlines the differentially private stochastic gradient descent method. Note that the gradient clipping used in this method effectively limits the contribution of each data-points towards the gradient, bounding the $\ell_2$ sensitivity and enabling the Gaussian mechanism with sub-sampling to be applied. Noting that the noise added scales with the clipping bound, large values of the clipping bound correspond to strong, but noisy gradient signals. A suggested setting for the clipping bound, $c_t$, is the median of the norms of unclipped gradients throughout the course of training. Furthermore, \citeauthor{moments_account} suggest using a learning rate which starts off at a high value large and is reduced over time. Additionally, it is remarked that $L \simeq \sqrt{N}$ is an appropriate setting for the lot size; larger values of $L$ improve the signal-to-noise ratio of the gradient estimate but increase the privacy cost. 

\begin{algorithm}
	\caption{Differentially Private Stochastic Gradient Descent (DP-SGD)}
	\label{alg:DP-SGD}
	\hspace*{\algorithmicindent} \textbf{Input:} Dataset $\altmathcal{D} = \lbrace \bm{x}_1, \ldots, \bm{x}_N \rbrace$, Loss function $\altmathcal{L}(\bm{\theta}) = \frac{1}{N} \sum_i \altmathcal{L}(\bm{\theta}, \bm{x}_i)$ \\
	\hspace*{\algorithmicindent} \textbf{Parameters:} Learning Rate $\alpha_t$, Clipping Bound $c_t$, Lot Size $L$, DP Noise Scale $\sigma_t$, \\ 	\hspace*{\algorithmicindent} Num. Iterations $T$ 
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\State Initialise $\bm{\theta}_0$ randomly. 
		\For {$t = 1, \ldots, T$}
		\State Take a random sample $L_t$ with sampling probability $q = L / N$
		\ForAll {$i \in L_t$} \Comment{Compute Gradient}
		\State $\bm{g}_t(\bm{x}_i) \leftarrow \nabla_{\bm{\theta}} \altmathcal{L}(\bm{\theta}, \bm{x}_i)$
		\State $\tilde{\bm{g}}_t(\bm{x}_i) \leftarrow \bm{g}_t(\bm{x}_i) / \max(||\bm{g}_t(\bm{x}_i)||_2/c_t, 1)$ \Comment{Gradient Clipping}
		\EndFor
				\State $\tilde{\bm{g}}_t \leftarrow \frac{1}{L} [\sum_{i \in L_t} \bm{g}_t(\bm{x}_i) + \sigma_t c_t \bm{z}]$ where $z_i \sim \altmathcal{N}(0, 1)$ \Comment{Perturb Clipped Gradient}
				\State $\bm{\theta}_t \leftarrow \bm{\theta}_{t-1} - \alpha_t \tilde{\bm{g}}_t$
		\EndFor
		\State \textbf{Output} $\bm{\theta}_T$ and calculate $(\epsilon, \delta)$ using the Moments Accountant. 
	\end{algorithmic}
\end{algorithm}

This technique has been applied to deep neural networks \citep{moments_account}. However, this algorithm can be used as a building block to create other differentially private techniques, and indeed this technique has been used to perform variational inference for non-conjugate models \citep{DPVI}. 

\section{Federated Learning}
\subsection{Partitioned Variational Inference}
\emph{Partitioned Variational Inference (PVI)} is a general framework which encompasses many variational Bayesian techniques. Assume that the dataset, $\altmathcal{D}$, has been partitioned into $M$ shards i.e. $\altmathcal{D} = \lbrace \bm{X}_1, \ldots, \bm{X}_M \rbrace$, where $\bm{X}_i = \lbrace \bm{x}_1, \ldots, \bm{x}_{N_i} \rbrace$. A probabilistic model with parameters $\bm{\theta}$ has been suggested to model this data with a known prior, $p(\bm{\theta})$, and likelihood function, $p(\bm{x}| \bm{\theta})$. The aim of Bayesian inference is to calculate the posterior density over the parameters, $p(\bm{\theta}| \altmathcal{D})$, but in general, it is not possible to compute this distribution. In variational Bayesian methods, a variational distribution, $q(\bm{\theta})$, is used to approximate the posterior. In this report, we refer to $q(\bm{\theta})$ as either the variational distribution or the approximate posterior. In PVI, this distribution takes the form:
\begin{align}
q(\bm{\theta}) = p(\bm{\theta}) \prod_{m=1}^M t_m(\bm{\theta}) \simeq \frac{1}{\altmathcal{Z}} p(\bm{\theta}) \prod_{m=1}^M p(\bm{X}_m|\bm{\theta}) = p(\bm{\theta}| \altmathcal{D})
\end{align}
Note that the variational distribution includes the product of terms corresponding to each client ($t_m(\bm{\theta})$), each of which approximates the (un-normalised) likelihood $p(\bm{X}_m | \bm{\theta})$. We refer to $t_m(\bm{\theta})$ as the approximate likelihood for client $m$. Additionally, note that the variational distribution does not include a normalising constant. 

\begin{algorithm}
	\caption{Partitioned Variational Inference (PVI)}
	\label{alg:PVI}
	\hspace*{\algorithmicindent} \textbf{Input:} Partitioned Dataset $\altmathcal{D} = \lbrace \bm{X}_1, \ldots, \bm{X}_M \rbrace$, Prior $p(\bm{\theta})$, Family of Distributions $\altmathcal{Q}$
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\State Initialise approximate likelihood: \begin{align}
			t_m^{(0)}(\bm{\theta}) \leftarrow 1\ \forall m 
		\end{align} 
		\State Initialise approximate posterior:
		\begin{align}
		q^{(0)}(\bm{\theta}) \leftarrow p(\bm{\theta})
		\end{align} 
		\For {$i = 1, 2, \ldots, \text{until convergence}$}
		\State $b_i \leftarrow $ index of next approximate likelihood to update
		\State Compute the new approximate likelihood:
		\begin{align}
		q^{(i)}(\bm{\theta}) \leftarrow  \mathop{\mathrm{argmax}}_{q(\bm{\theta}) \in \altmathcal{Q}} \int q(\bm{\theta}) \ln \frac{q^{(i-1)}(\bm{\theta}) p(\bm{X}_{b_i}|\bm{\theta})}{q(\bm{\theta}) t_{b_i}^{(i-1)}(\bm{\theta})}\ d\bm{\theta} \label{eq:local_free_energy_optimisation}
		\end{align}
		\State Update the approximate likelihood for the updated factor:
		\begin{align}
		t_{b_i}^{(i)}(\bm{\theta}) \leftarrow \frac{q^{(i)}(\bm{\theta})}{q^{(i-1)}(\bm{\theta})} t_{b_i}^{(i-1)}(\bm{\theta}) \label{eq:update_likelihood}
		\end{align}
		\EndFor
	\end{algorithmic}
\end{algorithm}

When partitioned variational inference is used in the federated learning context, each client stores its own data partition, $\bm{X}_m$, and communicates with a central parameter server which stores the global approximate posterior, $q(\bm{\theta})$. At each iteration, each client maximises a \emph{local} free energy \citep{PVI}. 

\begin{definition}[Local Free Energy]
	The local free energy is defined as:
	\begin{align}
	\altmathcal{F}_{b_i}^{(i)}(q(\bm{\theta})) = \int q(\bm{\theta}) \ln \frac{q^{(i-1)}(\bm{\theta}) p(\bm{X}_{b_i}|\bm{\theta})}{q(\bm{\theta}) t_{b_i}^{(i-1)}(\bm{\theta})}\ d\bm{\theta} \label{eq:def-local-free-energy}
	\end{align}
\end{definition}
It can be shown that the local free energy optimisation (Eq. \ref{eq:local_free_energy_optimisation}) is equivalent to the following $\altmathcal{KL}$ optimisation:
\begin{align}
q^{(i)}(\bm{\theta}) \leftarrow  \mathop{\mathrm{argmax}}_{q(\bm{\theta}) \in \altmathcal{Q}} \altmathcal{KL}(q(\bm{\theta}) || \hat{p}^{(i)}_{b_i}(\bm{\theta})) \label{eq:free_energy_kl}
\end{align}
where $\hat{p}^{(i)}_{b_i}(\bm{\theta})$ is known as the \emph{tilted distribution} and is defined as:
\begin{align}
\hat{p}^{(i)}_{b_i}(\bm{\theta}) &= \frac{1}{\altmathcal{Z}^{(i)}_{b_i}} \frac{q^{(i-1)}(\bm{\theta})}{t_{b_i}^{(i-1)}(\bm{\theta})} p(\bm{X}_{b_i}|\bm{\theta}) \\ &=  \frac{1}{\altmathcal{Z}^{(i)}_{b_i}} p(\bm{\theta}) p(\bm{X}_{b_i}| \bm{\theta}) \prod_{m \neq b_i} t_m^{(i-1)}(\bm{\theta})
\end{align}
which can be interpreted as a local estimate of the posterior for client $b_i$, using the exact local likelihood and approximate likelihood terms for the data partitions held at other clients. The free energy could be maximised in a number of ways, including analytical updates or gradient based methods (such as applying Algorithm \ref{alg:DP-SGD} on the negative local free energy).

In standard variational inference, a global free energy that depends on the entire dataset is maximised. 
\begin{definition}[Global Free Energy]
	The global free energy is defined as:
	\begin{align}
	\altmathcal{F}(q(\bm{\theta})) = \int q(\bm{\theta}) \ln \frac{p(\bm{\theta})\prod_{m=1}^M p(\bm{X}_m | \bm{\theta})}{q(\bm{\theta})}\ d\bm{\theta}
	\end{align}
\end{definition}
Maximising this free energy is equivalent to minimising the $\altmathcal{KL}$ divergence between the approximate posterior and the true posterior, $\altmathcal{KL}(q(\bm{\theta})|| p(\bm{\theta| \altmathcal{D}}))$ \citep{Bishop:2006}.

Now we return to the partitioned variational inference setting.
\begin{theorem}[Properties of PVI]
	Consider the approximate posterior at convergence, $q^*(\bm{\theta}) = p(\bm{\theta}) \prod_{m=1}^M t_m^{*}(\bm{\theta})$. Then:
	\begin{enumerate}
		\item The sum of local free energies is the global free energy:
		\begin{align}
		\sum_{m=1}^M \altmathcal{F}_m(q^*(\bm{\theta})) = \altmathcal{F}(q^*(\bm{\theta}))
		\end{align}
		\item Optimising the local free energies optimises the global free energy:
		\begin{align}
		q^*(\theta) = \mathop{\mathrm{argmax}}_{q(\bm{\theta}) \in \altmathcal{Q}} \altmathcal{F}_m(q(\bm{\theta}))\ \forall m \Rightarrow q^*(\theta) = \mathop{\mathrm{argmax}}_{q(\bm{\theta}) \in \altmathcal{Q}} \altmathcal{F}(q(\bm{\theta}))
		\end{align}
	\end{enumerate}
\end{theorem}

The above properties of PVI suggest that the algorithm should yield a good approximate posterior distribution \citep{PVI}.

\section{Additional Work}
\cite{heikkila2017differentially} develop differentially private Bayesian learning on distributed data for exponential conjugate family models. The posterior distributions obtained for these models depend on aggregated \emph{sufficient statistics} i.e. these statistics capture all available information about the parameters of the model in question. The approach taken in this paper is to calculate aggregate noisy sufficient statistics across a number of computation nodes, effectively applying the Gaussian mechanism with a distributed noise scheme. This can not be applied to other models.

\cite{geyer2017differentially} develop a technique to perform federated learning which does not protect single data-points held by each client but rather the entire dataset held by each client. This is performed by choosing a sub-set of clients to use to refine the model. Model updates are then calculated from each client, rescaled if required so the $\ell_2$ norm of each of the clients update is less than some value, $S$. This fixes the $\ell_2$-sensitivity, allowing the Gaussian mechanism (with noise scaling with $S$) to be applied centrally.