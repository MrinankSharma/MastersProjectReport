% ******************************* Introduction ********************************

\chapter{Conclusions}
%
%\ifpdf
%\graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
%\else
%\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
%\fi
\graphicspath{{Conclusions/Figs}}
The existing PVI algorithm was adapted in two different ways in order to create the data-point level and dataset level DP-PVI algorithms, which each have their own advantages and disadvantages. Data-point level protection makes fewer assumptions about the trustworthiness of external parties and enables each client to choose their own level of protection whilst the dataset level protection is the natural scheme to apply when each dataset corresponds to the data of a single user, but is less appropriate when the underlying data is inhomogeneous. 

These techniques were applied to a linear regression model with one parameter. With regards to data-point level protection, whilst DP-SGD was found to give close to non-private performance, the privacy guarantee achieved was poor. Clipped analytical updates were found to provide stronger privacy guarantees at the cost of bias in their variational distributions, and the choice of clipping bound is challenging in situations where we are unable to \emph{a priori} place bounds on data variables. A hybrid scheme, initially applying the clipped analytical updated followed by DP-SGD is likely give performance close to that achieved by DP-SGD at a significantly lower privacy cost. 

Dataset level DP-PVI was applied using exact analytical updates, and experiments were performed to identify a suitable value for the clipping bound. A small algorithmic modification is required to ensure that the privacy guarantees advertised by this scheme are legitimate. Unlike the data-point level protection scheme, if this bound is chosen too small no bias is introduced; only the number of iterations required to give convergence is affected. This is a very useful property. An appropriate choice of clipping bound and learning rate was identified for this scheme and these parameters were applied whilst varying the number of workers and points per worker, scaling the clipping bound with the number of points per worker. It was found that these settings were relatively robust to changes in these parameters, but they did tend to affect performance, and it is unclear the magnitude of gains which could be made by further tuning the algorithm hyper-parameters. Additionally, it was found that the choice of parametrisation is very important, and the properties of the parametrisation used in general will affect how robust a given set of parameter settings are to changes in the context in which the algorithm is applied; for the case study employed, the restriction of one of the parameters being positive introduced bias.

For the case study considered, we found that the dataset level DP-PVI algorithm was not only able to provide a stronger privacy guarantee than that of the data-point level DP-PVI algorithm (the same $(\epsilon, \delta)$ was achieved for both schemes) but also gave much better performance. 

Finally, we conclude in stating that the dataset level DP-PVI algorithm shows promise, and is able to provide excellent results for certain datasets and parameter settings. A small modification in the implementation will however be required to ensure correct privacy accounting, but it is suggested that this modification should not confer a large additional privacy cost. In general, there are a very large number of hyper-parameters which must be tuned, and unlike in the standard machine learning context, performing a search using the data which we wish to train on consumes the privacy budget itself! A machine learning practitioner wishing to apply this technique must take care in the parametrisation employed for the probabilistic model which is being trained and the value of clipping bound set, whilst the learning rate and the DP noise scale can be tuned more easily. 

\section{Future Work}
The application of differential privacy techniques to machine learning models is recent work, and therefore there remain a number of unanswered questions. 

Performance for a chosen $(\epsilon, \delta)$ privacy bound depends significantly on algorithm hyper-parameters, but it is not immediately clear how to choose these values or how changes in these values affect the optimal hyper-parameter settings, which is relevant as in practice, a designer would with choose hyper-parameter settings with a privacy guarantee in mind. In certain cases, we are unable to \emph{a priori} bound values meaning that appropriate scales for the clipping bound are difficult to obtain, and indeed performing investigations to determine these values will consume the fixed privacy budget. It is strongly suggested that a differentially private data-whitening scheme be applied to the data to mitigate this problem, but the privacy cost of applying this scheme must be accounted for.

Additionally, it is unclear how, in general, the choice of hyper-parameters generalises across several models and several data-sets. This will be influenced largely by the choice of parametrisation; for instance, in the case study employed, the value of $\theta$ has a large influence on the magnitude of the parameters, suggesting that it would also affect the optimal parameter settings. If it were possible to find parameter settings which were very robust across a wide range of datasets, fake data with similar underlying statistics or data for which privacy is not required could be used to find an appropriate set of hyper-parameters. It is likely that applying a differentially private data-whitening mechanism would improve the generalisation of model hyper-parameters. These settings could then be used to protect the sensitive dataset. Furthermore, a meta-analysis could be performed, for example, training a machine learning model to predict appropriate hyper-parameter settings from certain dataset characteristics. 

Furthermore, we note that schemes which adaptively modify the clipping bound, learning rate and noise scale may yield improvements and reduce the importance of \emph{a priori} choosing appropriate hyper-parameters. For the dataset DP-PVI algorithm, it is expected that updates early in the course of training are mostly moving the model towards the optimum but once the optimum is reached, the parameters of the variational distribution oscillate. Therefore, decreasing the learning rate as the number of iterations increases seems appropriate. For the DP-SGD approach, adaptive learning rate techniques which already exist could be applied directly, modifying them to be differentially private if required. It is not clear how an adaptive clipping bound scheme would work; as the results of the the analytical clipped updates for the data-point level DP-PVI scheme show, the clipping can fundamentally change the solutions obtained.

An alternative way to perform DP-PVI privately would be to construct a differentially private estimate of the local likelihood function, which would only need to be constructed when a client gathers new data. This likelihood function could then be fixed and used repeatedly in combination with exact analytical updates to yield a differentially private algorithm.

Furthermore, it is likely the applying the dataset level DP-PVI algorithm on inhomogeneous data will yield poor results since the magnitude of parameter updates may be very different. This needs to be investigated further. 